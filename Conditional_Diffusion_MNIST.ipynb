{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xnLBHLWyjZrV"},"outputs":[],"source":["from typing import Dict, Tuple\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import models, transforms\n","from torchvision.datasets import MNIST\n","from torchvision.utils import save_image, make_grid\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation, PillowWriter\n","import numpy as np\n","import glob\n","import os\n","from torch.utils.data import TensorDataset, DataLoader\n","import time\n","#from Conditional_Diffusion_MNIST.script import *"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6AuMXNmyhfh2"},"outputs":[],"source":["# @title Script from Conditional_Diffusion_MNIST\n","class ResidualConvBlock(nn.Module):\n","    def __init__(\n","        self, in_channels: int, out_channels: int, is_res: bool = False\n","    ) -> None:\n","        super().__init__()\n","        '''\n","        standard ResNet style convolutional block\n","        '''\n","        self.same_channels = in_channels==out_channels\n","        self.is_res = is_res\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        if self.is_res:\n","            x1 = self.conv1(x)\n","            x2 = self.conv2(x1)\n","            # this adds on correct residual in case channels have increased\n","            if self.same_channels:\n","                out = x + x2\n","            else:\n","                out = x1 + x2\n","            return out / 1.414\n","        else:\n","            x1 = self.conv1(x)\n","            x2 = self.conv2(x1)\n","            return x2\n","\n","\n","class UnetDown(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UnetDown, self).__init__()\n","        '''\n","        process and downscale the image feature maps\n","        '''\n","        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n","        self.model = nn.Sequential(*[ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)])\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","class UnetUp(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UnetUp, self).__init__()\n","        '''\n","        process and upscale the image feature maps\n","        '''\n","        layers = [\n","            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n","            ResidualConvBlock(out_channels, out_channels),\n","            ResidualConvBlock(out_channels, out_channels),\n","        ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x, skip):\n","        x = torch.cat((x, skip), 1)\n","        x = self.model(x)\n","        return x\n","\n","class EmbedFC(nn.Module):\n","    def __init__(self, input_dim, emb_dim):\n","        super(EmbedFC, self).__init__()\n","        '''\n","        generic one layer FC NN for embedding things\n","        '''\n","        self.input_dim = input_dim\n","        layers = [\n","            nn.Linear(input_dim, emb_dim),\n","            nn.GELU(),\n","            nn.Linear(emb_dim, emb_dim),\n","        ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.input_dim)\n","        return self.model(x)\n","\n","\n","class ContextUnet(nn.Module):\n","    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n","        super(ContextUnet, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.n_feat = n_feat\n","        self.n_classes = n_classes\n","\n","        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n","\n","        self.down1 = UnetDown(n_feat, n_feat)\n","        self.down2 = UnetDown(n_feat, 2 * n_feat)\n","\n","        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n","\n","        self.timeembed1 = EmbedFC(1, 2*n_feat)\n","        self.timeembed2 = EmbedFC(1, 1*n_feat)\n","        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n","        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n","\n","        self.up0 = nn.Sequential(\n","            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n","            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n","            nn.GroupNorm(8, 2 * n_feat),\n","            nn.ReLU(),\n","        )\n","\n","        self.up1 = UnetUp(4 * n_feat, n_feat)\n","        self.up2 = UnetUp(2 * n_feat, n_feat)\n","        self.out = nn.Sequential(\n","            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n","            nn.GroupNorm(8, n_feat),\n","            nn.ReLU(),\n","            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n","        )\n","\n","    def forward(self, x, c, t, context_mask):\n","        # x is (noisy) image, c is context label, t is timestep,\n","        # context_mask says which samples to block the context on\n","\n","        x = self.init_conv(x)\n","        down1 = self.down1(x)\n","        down2 = self.down2(down1)\n","        hiddenvec = self.to_vec(down2)\n","\n","        # convert context to one hot embedding\n","        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n","\n","        # mask out context if context_mask == 1\n","        context_mask = context_mask[:, None]\n","        context_mask = context_mask.repeat(1,self.n_classes)\n","        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n","        c = c * context_mask\n","\n","        # embed context, time step\n","        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n","        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n","        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n","        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n","\n","        # could concatenate the context embedding here instead of adaGN\n","        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n","\n","        up1 = self.up0(hiddenvec)\n","        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n","        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n","        up3 = self.up2(cemb2*up2+ temb2, down1)\n","        out = self.out(torch.cat((up3, x), 1))\n","        return out\n","\n","\n","def ddpm_schedules(beta1, beta2, T):\n","    \"\"\"\n","    Returns pre-computed schedules for DDPM sampling, training process.\n","    \"\"\"\n","    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n","\n","    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n","    sqrt_beta_t = torch.sqrt(beta_t)\n","    alpha_t = 1 - beta_t\n","    log_alpha_t = torch.log(alpha_t)\n","    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n","\n","    sqrtab = torch.sqrt(alphabar_t)\n","    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n","\n","    sqrtmab = torch.sqrt(1 - alphabar_t)\n","    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n","\n","    return {\n","        \"alpha_t\": alpha_t,  # \\alpha_t\n","        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n","        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n","        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n","        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n","        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n","        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n","    }\n","\n","class DDPM(nn.Module):\n","    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n","        super(DDPM, self).__init__()\n","        self.nn_model = nn_model.to(device)\n","\n","        # register_buffer allows accessing dictionary produced by ddpm_schedules\n","        # e.g. can access self.sqrtab later\n","        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n","            self.register_buffer(k, v)\n","\n","        self.n_T = n_T\n","        self.device = device\n","        self.drop_prob = drop_prob\n","        self.loss_mse = nn.MSELoss()\n","\n","    def forward(self, x, c):\n","        \"\"\"\n","        this method is used in training, so samples t and noise randomly\n","        \"\"\"\n","\n","        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n","        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n","\n","        x_t = (\n","            self.sqrtab[_ts, None, None, None] * x\n","            + self.sqrtmab[_ts, None, None, None] * noise\n","        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n","        # We should predict the \"error term\" from this x_t. Loss is what we return.\n","\n","        # dropout context with some probability\n","        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n","\n","        # return MSE between added noise, and our predicted noise\n","        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n","\n","    def sample(self, n_sample, size, device, guide_w = 0.0):\n","        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n","        # to make the fwd passes efficient, we concat two versions of the dataset,\n","        # one with context_mask=0 and the other context_mask=1\n","        # we then mix the outputs with the guidance scale, w\n","        # where w>0 means more guidance\n","\n","        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n","        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n","        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n","\n","        # don't drop context at test time\n","        context_mask = torch.zeros_like(c_i).to(device)\n","\n","        # double the batch\n","        c_i = c_i.repeat(2)\n","        context_mask = context_mask.repeat(2)\n","        context_mask[n_sample:] = 1. # makes second half of batch context free\n","\n","        x_i_store = [] # keep track of generated steps in case want to plot something\n","        print()\n","        for i in range(self.n_T, 0, -1):\n","            #print(f'sampling timestep {i}',end='\\r')\n","            t_is = torch.tensor([i / self.n_T]).to(device)\n","            t_is = t_is.repeat(n_sample,1,1,1)\n","\n","            # double batch\n","            x_i = x_i.repeat(2,1,1,1)\n","            t_is = t_is.repeat(2,1,1,1)\n","\n","            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n","\n","            # split predictions and compute weighting\n","            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n","            eps1 = eps[:n_sample]\n","            eps2 = eps[n_sample:]\n","            eps = (1+guide_w)*eps1 - guide_w*eps2\n","            x_i = x_i[:n_sample]\n","            x_i = (\n","                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n","                + self.sqrt_beta_t[i] * z\n","            )\n","            if i%20==0 or i==self.n_T or i<8:\n","                x_i_store.append(x_i.detach().cpu().numpy())\n","\n","        x_i_store = np.array(x_i_store)\n","        return x_i, x_i_store\n","\n","\n","def sample_new(self, n_sample, size, device, guide_w = 0.0):\n","        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n","        # to make the fwd passes efficient, we concat two versions of the dataset,\n","        # one with context_mask=0 and the other context_mask=1\n","        # we then mix the outputs with the guidance scale, w\n","        # where w>0 means more guidance\n","\n","        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n","        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n","        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n","\n","        # don't drop context at test time\n","        context_mask = torch.zeros_like(c_i).to(device)\n","\n","        # double the batch\n","        c_i = c_i.repeat(2)\n","        context_mask = context_mask.repeat(2)\n","        context_mask[n_sample:] = 1. # makes second half of batch context free\n","\n","        print()\n","        for i in range(self.n_T, 0, -1):\n","            print(f'sampling timestep {i}',end='\\r')\n","            t_is = torch.tensor([i / self.n_T]).to(device)\n","            t_is = t_is.repeat(n_sample,1,1,1)\n","\n","            # double batch\n","            x_i = x_i.repeat(2,1,1,1)\n","            t_is = t_is.repeat(2,1,1,1)\n","\n","            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n","\n","            # split predictions and compute weighting\n","            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n","            eps1 = eps[:n_sample]\n","            eps2 = eps[n_sample:]\n","            eps = (1+guide_w)*eps1 - guide_w*eps2\n","            x_i = x_i[:n_sample]\n","            x_i = (\n","                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n","                + self.sqrt_beta_t[i] * z\n","            )\n","\n","        return x_i"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R87o1ctPl5tW"},"outputs":[],"source":["def train(Version, Guidance,\n","          n_epoch = 20, batch_size = 256,\n","          n_T = 400 , device = \"cuda:0\", n_feat = 128 , lrate = 1e-4, ):\n","    def load_data(Version, Guidance):\n","      all_files = glob.glob(os.path.join(\"\", f\"./Results_experiments/MNIST/Version_{Version-1}_Guidance_{Guidance}/*.npy\"))\n","      combined_data = np.concatenate(([np.load(fname) for fname in all_files]))\n","      return combined_data\n","\n","    ####\n","    #Modelo\n","    ####\n","    ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=10), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n","    ddpm.to(device)\n","\n","    ####\n","    #DATASET\n","    ####\n","\n","    if Version <1:\n","      return \"\"\n","    elif Version == 1:\n","      tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n","      dataset = MNIST(\"salida\", train=True, download=True, transform=tf)\n","    else:\n","      X =  load_data(Version, Guidance)[:60000]\n","      Y = np.array(list(range(10))*6000)\n","      dataset = TensorDataset( torch.Tensor(X), torch.from_numpy(Y))\n","\n","\n","    ####\n","    #Entrenamiento\n","    ####\n","    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n","    for ep in range(n_epoch):\n","        print(f'epoch {ep}')\n","        ddpm.train()\n","        # linear lrate decay\n","        optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n","        pbar = tqdm(dataloader)\n","        loss_ema = None\n","        for x, c in pbar:\n","            optim.zero_grad()\n","            x = x.to(device)\n","            c = c.to(device)\n","            loss = ddpm(x, c)\n","            loss.backward()\n","            if loss_ema is None:\n","                loss_ema = loss.item()\n","            else:\n","                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n","            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n","            optim.step()\n","\n","        ddpm.eval()\n","        #if ep == int(n_epoch-1):\n","    torch.save(ddpm.state_dict(), f\"./Results_experiments/MNIST/Model_{Version}_{Guidance}.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NTBHqmR0ZlXS"},"outputs":[],"source":["def Generation(Version, Guidance, device=\"cuda:0\", generating_steps=600, generated_in_step=10):\n","  ####\n","  #Model\n","  ####\n","  ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=128, n_classes=10), betas=(1e-4, 0.02), n_T=400, device=device, drop_prob=0.1)\n","  ddpm.to(device)\n","  ddpm.load_state_dict(torch.load(f\"./Results_experiments/MNIST/Model_{Version}_{Guidance}.pth\"))\n","  ddpm.eval()\n","\n","  ####\n","  #Generation\n","  ####\n","  with torch.no_grad():\n","    for _ in range(generating_steps):\n","      x_gen, _ = ddpm.sample(10*generated_in_step, (1, 28, 28), device, guide_w=Guidance)\n","      x_gen = x_gen.cpu().data.numpy()\n","      np.save(f'./Results_experiments/MNIST/Version_{Version}_Guidance_{Guidance}/Generated_MNIST_{int(time.time())}', x_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wiGBO8vKFfb4"},"outputs":[],"source":["for version in range(1, 10):\n","  print(version)\n","  train(version, 1)\n","  Generation(version, 1)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1v3Qx_xdS96ZNcXvdygNSUY4QLxEdKKZy","timestamp":1686048327161}],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}